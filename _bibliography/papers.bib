@inproceedings{mulligan_compositional_2022,
  title = {A Compositional Semantics for Spatial Perspective-Shifting Adjuncts},
  booktitle = {Proceedings of the 40th {{West Coast Conference}} on {{Formal Linguistics}}},
  author = {Mulligan, Karl and Rawlins, Kyle},
  date = {2022},
  location = {{Stanford, California}},
  abstract = {This paper gives a compositional account of a construction which shifts perspective in projective prepositions such as <i>behind</i> and <i>left of</i>: the left-adjoined <i>from</i>-adjunct, as in "From the door, the ball is behind the chair." The implicit argument for perspective is normally assigned its value anaphorically, but in case of shifting with <i>from</i>-adjuncts, this variable is instead bound. The binding operator introduced by left-adjoined <i>from</i> is analyzed as a generic quantifier over simulated individuals, in the sense of Moltmann (2006), with the locational information in the <i>from</i>-adjunct forming the content of the restrictor. This paper also analyzes perspective shifting with a truth-conditionally equivalent expression, the conditional paraphrase "If you're at the door," using a similar approach.},
  eventtitle = {{{WCCFL}}},
  annotation = {(to appear)},
  preprint = {wccfl40.pdf} 
}

@inproceedings{mulligan_structure_2021-1,
  title = {Structure {{Here}}, {{Bias There}}: {{Hierarchical Generalization}} by {{Jointly Learning Syntactic Transformations}}},
  shorttitle = {Structure {{Here}}, {{Bias There}}},
  booktitle = {Proceedings of the {{Society}} for {{Computation}} in {{Linguistics}}},
  author = {Mulligan, Karl and Frank, Robert and Linzen, Tal},
  date = {2021-01-01},
  volume = {4},
  pages = {125--135},
  url = {https://scholarworks.umass.edu/scil/vol4/iss1/13},
  abstract = {When learning syntactic transformations, children consistently induce structure-dependent generalizations, even though the primary linguistic data may be consistent with both linear and hierarchical rules. What is the source of this inductive bias? In this paper, we use computational models to investigate the hypothesis that evidence for the structure-sensitivity of one syntactic transformation can bias the acquisition of another transformation in favor of a hierarchical rule. We train sequence-to-sequence models based on artificial neural networks to learn multiple syntactic transformations at the same time in a fragment of English; we hold out cases that disambiguate linear and hierarchical rules for one of those transformations, and then test for hierarchical generalization to these held-out sentence types. Consistent with our hypothesis, we find that multitask learning induces a hierarchical bias for certain combinations of tasks, and that this bias is stronger for transformations that share computational building blocks. At the same time, the bias is in general insufficient to lead the learner to categorically acquire the hierarchical generalization for the target transformation.},
  pdf = {scil2021.pdf}
}

@article{specia_grounded_2020,
  title = {Grounded {{Sequence}} to {{Sequence Transduction}}},
  author = {Specia, Lucia and Barrault, Loic and Caglayan, Ozan and Duarte, Amanda and Elliott, Desmond and Gella, Spandana and Holzenberger, Nils and Lala, Chiraag and Lee, Sun Jae and Libovicky, Jindrich and Madhyastha, Pranava and Metze, Florian and Mulligan, Karl and Ostapenko, Alissa and Palaskar, Shruti and Sanabria, Ramon and Wang, Josiah and Arora, Raman},
  date = {2020-03},
  journaltitle = {IEEE Journal of Selected Topics in Signal Processing},
  volume = {14},
  number = {3},
  pages = {577--591},
  url = {https://ieeexplore.ieee.org/document/9103248},
  issn = {1941-0484},
  doi = {10.1109/JSTSP.2020.2998415},
  abstract = {Speech recognition and machine translation have made major progress over the past decades, providing practical systems to map one language sequence to another. Although multiple modalities such as sound and video are becoming increasingly available, the state-of-the-art systems are inherently unimodal, in the sense that they take a single modality - either speech or text - as input. Evidence from human learning suggests that additional modalities can provide disambiguating signals crucial for many language tasks. In this article, we describe the How2 dataset, a large, open-domain collection of videos with transcriptions and their translations. We then show how this single dataset can be used to develop systems for a variety of language tasks and present a number of models meant as starting points. Across tasks, we find that building multimodal architectures that perform better than their unimodal counterpart remains a challenge. This leaves plenty of room for the exploration of more advanced solutions that fully exploit the multimodal nature of the How2 dataset, and the general direction of multimodal learning with other datasets as well.},
  eventtitle = {{{IEEE Journal}} of {{Selected Topics}} in {{Signal Processing}}},
  keywords = {Adaptation models,Feature extraction,Grounding,machine translation,multimodal machine learning,representation learning,speech recognition,Speech recognition,summarization,Task analysis,Training,Visualization},
  pdf = {jsalt_ieee.pdf}
}
